{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微积分-求导\n",
    "\n",
    "\n",
    "使用tensorflow的GradientTape实现自动求导\n",
    "\n",
    "\n",
    "## 安装tensorflow\n",
    "\n",
    "\n",
    "pip\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "\n",
    "conda\n",
    "```bash\n",
    "conda install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单的求导\n",
    "\n",
    "\n",
    "对 x<sup>2</sup> 求导，是 2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25.0, shape=(), dtype=float32) tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(5.0) # 假设x=5\n",
    "with tf.GradientTape() as tape:\n",
    "  y = tf.square(x)\n",
    "y_grad = tape.gradient(y, x)\n",
    "print(y, y_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 偏导\n",
    "\n",
    "\n",
    "L(w, b) = || X*w + b -y ||<sup>2</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1., 2.],[ 3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable([[1.], [2.]])\n",
    "b = tf.Variable(1.)\n",
    "with tf.GradientTape() as tape:\n",
    "  L = tf.reduce_sum(tf.square(tf.matmul(x, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])\n",
    "print(L, w_grad, b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高阶求导\n",
    "\n",
    "\n",
    "对 x<sup>3</sup> 求导，是 3x<sup>2</sup> 再求导，是 6x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(75.0, shape=(), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(5.0) # 假设x=5\n",
    "with tf.GradientTape() as tape1:\n",
    "  with tf.GradientTape() as tape0:\n",
    "    y = x ** 3\n",
    "  y_grad0 = tape0.gradient(y, x)\n",
    "y_grad1 = tape1.gradient(y_grad0, x)\n",
    "print(y, y_grad0, y_grad1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 持续\n",
    "\n",
    "\n",
    "GradientTape比较特殊，跟之前我们理解的with是不一样的，执行一次之后就释放\n",
    "\n",
    "\n",
    "> 下面的做法就是错误的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1840c066bef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_grad1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_grad2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_grad1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_grad2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \"\"\"\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to\"\n\u001b[0m\u001b[1;32m   1028\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0) # x1=2\n",
    "x2 = tf.Variable(3.0) # x2=3\n",
    "with tf.GradientTape() as tape:\n",
    "  y1 = tf.square(x1)\n",
    "  y2 = tf.pow(x2, 3)\n",
    "y_grad1 = tape.gradient(y1, x1)\n",
    "y_grad2 = tape.gradient(y2, x2)\n",
    "print(y1, y_grad1, y2, y_grad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要想执行多个，必须用持续模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(6.0, shape=(), dtype=float32) tf.Tensor(64.0, shape=(), dtype=float32) tf.Tensor(48.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(3.0)\n",
    "x2 = tf.Variable(4.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y1 = tf.square(x1)\n",
    "  y2 = tf.pow(x2, 3)\n",
    "y_grad1 = tape.gradient(y1, x1)\n",
    "y_grad2 = tape.gradient(y2, x2)\n",
    "print(y1, y_grad1, y2, y_grad2)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际应用\n",
    "\n",
    "\n",
    "回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.7083977> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.20608652>\n"
     ]
    }
   ],
   "source": [
    "x_raw = np.array([2013,2014,2015,2016,2017], dtype=np.float32)\n",
    "y_raw = np.array([12000,14000,15000,16500,17500], dtype=np.float32)\n",
    "x = (x_raw - x_raw.min()) / (x_raw.max() - x_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "w = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "variables = [w, b]\n",
    "num_epoch = 1000\n",
    "optimizer = tf.keras.optimizers.SGD(1e-3)\n",
    "for e in range(num_epoch):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_hat = w * x + b\n",
    "    loss = tf.reduce_sum(tf.square(y_hat-y))\n",
    "  grads = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(grads, variables))\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到结果\n",
    "\n",
    "\n",
    "y = 0.7083977x + 0.20608652\n",
    "\n",
    "\n",
    "epoch越高，数值会越好。\n",
    "\n",
    "\n",
    "这个结果其实并不如机器学习里直接数学算出来的好，但是深度学习大部分都是不能直接推出来结果，需要训练"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "596906d7856812f195e1bde6460efa676a5566064988986caf2166ec5e5080a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "596906d7856812f195e1bde6460efa676a5566064988986caf2166ec5e5080a0"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
