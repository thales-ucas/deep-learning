{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微积分-求导\n",
    "\n",
    "\n",
    "使用tensorflow的GradientTape实现自动求导\n",
    "\n",
    "\n",
    "## 安装tensorflow\n",
    "\n",
    "\n",
    "pip\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "\n",
    "conda\n",
    "```bash\n",
    "conda install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入tensorflow模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单的求导\n",
    "\n",
    "\n",
    "对 x<sup>2</sup> 求导，是 2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25.0, shape=(), dtype=float32) tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(5.0) # 假设x=5\n",
    "with tf.GradientTape() as tape:\n",
    "  y = tf.square(x)\n",
    "y_grad = tape.gradient(y, x)\n",
    "print(y, y_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 偏导\n",
    "\n",
    "\n",
    "L(w, b) = || X*w + b -y ||<sup>2</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1., 2.],[ 3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable([[1.], [2.]])\n",
    "b = tf.Variable(1.)\n",
    "with tf.GradientTape() as tape:\n",
    "  L = tf.reduce_sum(tf.square(tf.matmul(x, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])\n",
    "print(L, w_grad, b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高阶求导\n",
    "\n",
    "\n",
    "对 x<sup>3</sup> 求导，是 3x<sup>2</sup> 再求导，是 6x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(75.0, shape=(), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(5.0) # 假设x=5\n",
    "with tf.GradientTape() as tape1:\n",
    "  with tf.GradientTape() as tape0:\n",
    "    y = x ** 3\n",
    "  y_grad0 = tape0.gradient(y, x)\n",
    "y_grad1 = tape1.gradient(y_grad0, x)\n",
    "print(y, y_grad0, y_grad1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 持续\n",
    "\n",
    "\n",
    "GradientTape比较特殊，跟之前我们理解的with是不一样的，执行一次之后就释放\n",
    "\n",
    "\n",
    "> 下面的做法就是错误的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\thales\\ucas\\deep-learning\\tape.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thales/ucas/deep-learning/tape.ipynb#ch0000009?line=4'>5</a>\u001b[0m   y2 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mpow(x2, \u001b[39m3\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thales/ucas/deep-learning/tape.ipynb#ch0000009?line=5'>6</a>\u001b[0m y_grad1 \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(y1, x1)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/thales/ucas/deep-learning/tape.ipynb#ch0000009?line=6'>7</a>\u001b[0m y_grad2 \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(y2, x2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thales/ucas/deep-learning/tape.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(y1, y_grad1, y2, y_grad2)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1032\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1001'>1002</a>\u001b[0m \u001b[39m\"\"\"Computes the gradient using operations recorded in context of this tape.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1002'>1003</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1003'>1004</a>\u001b[0m \u001b[39mNote: Unless you set `persistent=True` a GradientTape can only be used to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1028'>1029</a>\u001b[0m \u001b[39m   called with an unknown value.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1029'>1030</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1030'>1031</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1031'>1032</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mA non-persistent GradientTape can only be used to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1032'>1033</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mcompute one set of gradients (or jacobians)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1033'>1034</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recording:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/tf2/lib/site-packages/tensorflow/python/eager/backprop.py?line=1034'>1035</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0) # x1=2\n",
    "x2 = tf.Variable(3.0) # x2=3\n",
    "with tf.GradientTape() as tape:\n",
    "  y1 = tf.square(x1)\n",
    "  y2 = tf.pow(x2, 3)\n",
    "y_grad1 = tape.gradient(y1, x1)\n",
    "y_grad2 = tape.gradient(y2, x2)\n",
    "print(y1, y_grad1, y2, y_grad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要想执行多个，必须用持续模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(6.0, shape=(), dtype=float32) tf.Tensor(64.0, shape=(), dtype=float32) tf.Tensor(48.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(3.0)\n",
    "x2 = tf.Variable(4.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y1 = tf.square(x1)\n",
    "  y2 = tf.pow(x2, 3)\n",
    "y_grad1 = tape.gradient(y1, x1)\n",
    "y_grad2 = tape.gradient(y2, x2)\n",
    "print(y1, y_grad1, y2, y_grad2)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际应用\n",
    "\n",
    "\n",
    "回归\n",
    "\n",
    "\n",
    "这里用到了keras还需要安装keras\n",
    "\n",
    "\n",
    "```bash\n",
    "conda install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.7083977> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.20608652>\n"
     ]
    }
   ],
   "source": [
    "x_raw = np.array([2013,2014,2015,2016,2017], dtype=np.float32)\n",
    "y_raw = np.array([12000,14000,15000,16500,17500], dtype=np.float32)\n",
    "x = (x_raw - x_raw.min()) / (x_raw.max() - x_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "w = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "variables = [w, b]\n",
    "num_epoch = 1000\n",
    "optimizer = tf.keras.optimizers.SGD(1e-3)\n",
    "for e in range(num_epoch):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_hat = w * x + b\n",
    "    loss = tf.reduce_sum(tf.square(y_hat-y))\n",
    "  grads = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(grads, variables))\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到结果\n",
    "\n",
    "\n",
    "y = 0.7083977x + 0.20608652\n",
    "\n",
    "\n",
    "epoch越高，数值会越好。\n",
    "\n",
    "\n",
    "这个结果其实并不如机器学习里直接数学算出来的好，但是深度学习大部分都是不能直接推出来结果，需要训练"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "596906d7856812f195e1bde6460efa676a5566064988986caf2166ec5e5080a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "596906d7856812f195e1bde6460efa676a5566064988986caf2166ec5e5080a0"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
